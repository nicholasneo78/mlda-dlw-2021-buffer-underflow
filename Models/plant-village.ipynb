{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-16T05:07:42.760672Z","iopub.execute_input":"2021-10-16T05:07:42.761069Z","iopub.status.idle":"2021-10-16T05:07:42.786258Z","shell.execute_reply.started":"2021-10-16T05:07:42.760978Z","shell.execute_reply":"2021-10-16T05:07:42.785552Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport cv2","metadata":{"execution":{"iopub.status.busy":"2021-10-16T05:07:43.465533Z","iopub.execute_input":"2021-10-16T05:07:43.465904Z","iopub.status.idle":"2021-10-16T05:07:49.407530Z","shell.execute_reply.started":"2021-10-16T05:07:43.465867Z","shell.execute_reply":"2021-10-16T05:07:49.406810Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"%pip install split_folders\nimport splitfolders\ninput_folder = \"../input/plantvillage-dataset/color\"\noutput = \"./output\"\n\nsplitfolders.ratio(input_folder, output=output, seed=42, ratio=(.8, .1, .1)) # ratio of split are in order of train/val/test. You can change to whatever you want. For train/val sets only, you could do .75, .25 for example.","metadata":{"execution":{"iopub.status.busy":"2021-10-16T05:07:49.409184Z","iopub.execute_input":"2021-10-16T05:07:49.409452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import cv2\n\nimg1 = cv2.imread('./output/train/Blueberry___healthy/e623ffc6-6ad2-43b4-84df-bd3a8ca46a14___RS_HL 0412.JPG')\nprint(img1.shape)\n\nplt.figure()\nplt.imshow(img1) \nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"datagen = ImageDataGenerator()\ntrain_generator = datagen.flow_from_directory(\n                  directory='./output/train/',\n                  target_size=(256, 256), # resize to this size\n                  color_mode=\"rgb\", # for coloured images\n                  batch_size=1, # number of images to extract from folder for every batch\n                  class_mode=\"categorical\", # classes to predict\n                  seed=2021 # to make the result reproducible\n                  )\nvalidation_generator = datagen.flow_from_directory(\n                  directory='./output/val/',\n                  target_size=(256, 256), # resize to this size\n                  color_mode=\"rgb\", # for coloured images\n                  batch_size=1, # number of images to extract from folder for every batch\n                  class_mode=\"categorical\", # classes to predict\n                  seed=2021 # to make the result reproducible\n                  )\ntest_generator = datagen.flow_from_directory(\n                  directory='./output/test/',\n                  target_size=(256, 256), # resize to this size\n                  color_mode=\"rgb\", # for coloured images\n                  batch_size=1, # number of images to extract from folder for every batch\n                  class_mode=\"categorical\", # classes to predict\n                  seed=2021 # to make the result reproducible\n                  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\n\nbase_model = keras.applications.Xception(weights='imagenet', input_shape=(256, 256, 3),include_top=False)\nbase_model.trainable = True\ninputs = keras.Input(shape=(256, 256, 3))\nx = base_model(inputs, training=True)\nx = keras.layers.GlobalAveragePooling2D()(x)\noutputs = keras.layers.Dense(38)(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.compile(optimizer=keras.optimizers.Adam(),\n              loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n              metrics=[keras.metrics.CategoricalAccuracy()])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import numpy as np\n# import tensorflow as tf\n# from tensorflow import keras\n\n# base_model = keras.applications.Xception(weights='imagenet', input_shape=(256, 256, 3),include_top=False)\n# base_model.trainable = True\n# inputs = keras.Input(shape=(256, 256, 3))\n# x = base_model(inputs, training=True)\n# x = keras.layers.GlobalAveragePooling2D()(x)\n# outputs = keras.layers.Dense(38,activation='softmax')(x)\n# model = keras.Model(inputs, outputs)\n\n# model.compile(optimizer=keras.optimizers.Adam(),\n#               loss=keras.losses.CategoricalCrossentropy(),\n#               metrics=[keras.metrics.CategoricalAccuracy()])","metadata":{"execution":{"iopub.status.busy":"2021-10-16T02:49:04.674745Z","iopub.execute_input":"2021-10-16T02:49:04.675314Z","iopub.status.idle":"2021-10-16T02:49:06.041505Z","shell.execute_reply.started":"2021-10-16T02:49:04.675275Z","shell.execute_reply":"2021-10-16T02:49:06.040798Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 32\nn_epochs = 5\nmodel_history = model.fit(train_generator,\n                            batch_size=BATCH_SIZE,\n                            epochs=n_epochs,\n                            validation_data=validation_generator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.save(\"my_Xception_model_total_softmax.h5\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.models import load_model\n \n# load model\nmodel = load_model('../input/plant-village-xception/my_Xception_model.h5')\n# summarize model.\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2021-10-16T01:15:46.520815Z","iopub.execute_input":"2021-10-16T01:15:46.521134Z","iopub.status.idle":"2021-10-16T01:15:52.631835Z","shell.execute_reply.started":"2021-10-16T01:15:46.521093Z","shell.execute_reply":"2021-10-16T01:15:52.631128Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"x = model.evaluate(test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T01:46:34.781395Z","iopub.execute_input":"2021-10-16T01:46:34.781652Z","iopub.status.idle":"2021-10-16T01:47:43.846053Z","shell.execute_reply.started":"2021-10-16T01:46:34.781622Z","shell.execute_reply":"2021-10-16T01:47:43.845251Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(x)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T01:48:55.246538Z","iopub.execute_input":"2021-10-16T01:48:55.246809Z","iopub.status.idle":"2021-10-16T01:48:55.251740Z","shell.execute_reply.started":"2021-10-16T01:48:55.246778Z","shell.execute_reply":"2021-10-16T01:48:55.250826Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(test_generator)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T01:49:23.052145Z","iopub.execute_input":"2021-10-16T01:49:23.052872Z","iopub.status.idle":"2021-10-16T01:50:12.442475Z","shell.execute_reply.started":"2021-10-16T01:49:23.052830Z","shell.execute_reply":"2021-10-16T01:50:12.441699Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"predicted_class_indices = np.argmax(pred, axis=1)\nprint(predicted_class_indices)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T01:55:32.036361Z","iopub.execute_input":"2021-10-16T01:55:32.036627Z","iopub.status.idle":"2021-10-16T01:55:32.041924Z","shell.execute_reply.started":"2021-10-16T01:55:32.036598Z","shell.execute_reply":"2021-10-16T01:55:32.041077Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"labels = test_generator.class_indices\nprint(labels)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T02:22:14.737850Z","iopub.execute_input":"2021-10-16T02:22:14.738369Z","iopub.status.idle":"2021-10-16T02:22:14.742717Z","shell.execute_reply.started":"2021-10-16T02:22:14.738332Z","shell.execute_reply":"2021-10-16T02:22:14.742030Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"print(len(test_generator), len(predicted_class_indices))","metadata":{"execution":{"iopub.status.busy":"2021-10-16T02:16:09.013465Z","iopub.execute_input":"2021-10-16T02:16:09.013723Z","iopub.status.idle":"2021-10-16T02:16:09.022552Z","shell.execute_reply.started":"2021-10-16T02:16:09.013693Z","shell.execute_reply":"2021-10-16T02:16:09.018147Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"test_img = os.listdir('./output/test/Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot/')\nprint(test_img)","metadata":{"execution":{"iopub.status.busy":"2021-10-16T02:20:54.727882Z","iopub.execute_input":"2021-10-16T02:20:54.728187Z","iopub.status.idle":"2021-10-16T02:20:54.736810Z","shell.execute_reply.started":"2021-10-16T02:20:54.728153Z","shell.execute_reply":"2021-10-16T02:20:54.735944Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.preprocessing import image\nimg = image.load_img('./output/test/Corn_(maize)___Cercospora_leaf_spot Gray_leaf_spot/75d924d6-a630-4e26-9e74-5b1e6dfc5e22___RS_GLSp 4475.JPG')\nimg_array = image.img_to_array(img)\nimg_batch = np.expand_dims(img_array, axis=0)\nprint(img_batch.shape)\np = model.predict(img_batch)\n# print(p)\nprint(label,np.argmax(p, axis=1))","metadata":{"execution":{"iopub.status.busy":"2021-10-16T02:38:57.063540Z","iopub.execute_input":"2021-10-16T02:38:57.063824Z","iopub.status.idle":"2021-10-16T02:38:57.140471Z","shell.execute_reply.started":"2021-10-16T02:38:57.063794Z","shell.execute_reply":"2021-10-16T02:38:57.138812Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}